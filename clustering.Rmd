---
#title: "Clustering"
author : F. Michel-Sendis
date: "`r Sys.Date()`"
output:
  html_document:
    toc : true
    toc_float: true 
    toc_depth: 3
---

```{r setup, include = FALSE} 
knitr::opts_chunk$set(
  fig.width=9, fig.height=6,
  echo = FALSE,
  message = FALSE,
  collapse = TRUE,
  comment = "#>"
)
#library(VIM)
library(tidyverse)
library(ggplot2)
library(plotly)
library(scales)
#library(sendis)
library(stats)
library(kableExtra)
library(factoextra)
library(randomForest)
library(rattle)
library(rpart)
library(rpart.plot)
library(splitstackshape)
library(data.table)

```

<br>

# Classification of SF data 

Machine Learning examples are applied to an example of spent nuclear fuel data. 

## The SFCOMPO-2.0 dataset

[SFCOMPO-2.0](http://www.oecd-nea.org/sfcompo) is the OECD NEA database of spent nuclear fuel assay data (Michel-Sendis, 2017).

```{r, eval=FALSE}

#Making-of sfcompo dataset : 

sf<-fread("../sendis/data-raw/sfcompo2.csv", stringsAsFactors = TRUE)%>%
  select(
    # -"Reactor design",
    # -"Assembly identifier",
    # -"Rod identifier",
    -"Sample identifier",
    -'Concentration', #redundant
    -'Concentration Unit')%>%
  rename(RNAME = 'Reactor name',
         RTYPE = 'Reactor type',
         EBUP = 'Estimated burnup',
         EU235 = 'e. U235',
         EPU = 'e. Pu',
         EPUFISS = 'e. Pu239 + Pu241',
         SAMPLEID = 'SFCompo sample ref',
         MTYPE = 'Measurement type',
         ITEM = 'Item',
         VALUE = 'Value',
         VALUNIT = 'Unit',
         METHOD = 'Method',
         LAB = 'Laboratory'
         )



 sf<-cSplit(as.data.table(sf), "EBUP", " ")%>%
   rename(
     EBUPVAL  = EBUP_1,
     EBUPUNIT = EBUP_2
     ) 
 
sf<-cSplit(as.data.table(sf), "Uncertainty", " ")%>%
   rename(
     UNCERTAINTY = Uncertainty_1
     )%>%
  select(-Uncertainty_2)%>%
  mutate(VALERR = UNCERTAINTY/Sigma,
         UNCERTAINTY = NULL, 
         Sigma = NULL)%>%
  filter(!is.na(VALERR))
 
sf<-cSplit(as.data.table(sf), "EU235", " ")%>%
   rename(
     EU235  = EU235_1
     )%>%
  select(-EU235_2)

sf<-cSplit(as.data.table(sf), "EPU", " ")%>%
   rename(
     EPU  = EPU_1
     )%>%
  select(-EPU_2)

sf<-cSplit(as.data.table(sf), "EPUFISS", " ")%>%
   rename(
     EPUFISS  = EPUFISS_1
     )%>%
  select(-EPUFISS_2)

sf<-sf%>%
  select(
    SAMPLEID,
    RNAME,RTYPE,
    EU235,EPU,EPUFISS,
    EBUPVAL,EBUPUNIT,
    MTYPE, ITEM,
    Z,A,I,
    VALUE,VALUNIT,VALERR,
    METHOD, 
    LAB
    )%>%
  mutate(VALUE=as.numeric(VALUE))

# Getting rid of useless data and transformong to same units: 
 sf<-sf%>%
  filter(
    ITEM!="Burnup",
    EBUPUNIT!="MW*h/kgUi"
         )%>%
   mutate(
     EBUPVAL=ifelse(EBUPUNIT=='MW*d/tUi'  , EBUPVAL/1000,
              ifelse(EBUPUNIT=='MW*d/tHMi', EBUPVAL/1000, 
              EBUPVAL
     )))%>%
  rename(EBUP=EBUPVAL)%>%
  select(-EBUPUNIT) # ALL burnups now given in GWD/tHMi
   
summary(sf)

ics<-sf%>%filter(MTYPE=="Isotopic Concentration")
 
t<-fread('../sendis/data-raw/sfcompo_converted.csv', stringsAsFactors = TRUE, sep = ",")%>%
  select(-BupUnit,-'Assembly identifier', -'Rod identifier', 
         -'Sample identidier', -'Unit*')%>%
  rename(RNAME = 'Reactor name',
         RCODE = 'Reactor code',
         SAMPLEID = 'SFCompo sample ref',
         EBUP = 'Burnup',
         ITEM = 'Nuclide',
         VALUE = 'Concentration',
         VALUNIT = 'ConcUnit',
         CONCVAL = 'Concentration*'
         )

t<-cSplit(as.data.table(t), "Uncertainty", " ")%>%
   rename(
     UNCERTAINTY = Uncertainty_1
     )%>%
  select(-Uncertainty_2)%>%
  mutate(CONCERRPCT = UNCERTAINTY/Sigma,
         UNCERTAINTY = NULL, 
         Sigma = NULL)%>%
group_by(SAMPLEID, ITEM)%>%
  mutate(
    MVALUE=mean(CONCVAL),
    MERR=sqrt(mean((CONCERRPCT/100)^2*CONCVAL^2))
         )%>%
  select(-Z, -A, -I, -VALUE, -VALUNIT, -CONCVAL, -CONCERRPCT, -EBUP)%>%
  arrange(SAMPLEID, ITEM)%>%
  unique()

#make data long :
t<-t%>%
  spread(key = ITEM, value = MVALUE)%>%
  select(-RNAME,-RCODE, -MERR)%>%
  rename(
    Am0 = "Am (natural)",
    Cm0 = "Cm (natural)",
    Cs0 = "Cs (natural)",
    Eu0 = "Eu (natural)",
    Gd0 = "Gd (natural)",
    Nd0 = "Nd (natural)",
    Pu0 = "Pu (natural)",
    Sm0 = "Sm (natural)",
    U0 = "U (natural)" 
  )


# Extremely important : group_by and summarize : 
t[is.na(t)] <- 0
t<-t%>% group_by(SAMPLEID) %>% summarise_all(funs(sum))


samples_info<-sf%>%select(SAMPLEID, RNAME, RTYPE, starts_with("E"))%>%unique()
#replace all initial enrichment NA's with 0 : 
samples_info[is.na(samples_info)] <- 0

sf_data<-merge(t,samples_info)%>%
  select(-RNAME)%>%
  na.omit() #get rid of 12% of data without uncertainty :
 
head(sf_data)

# save to csv file : 
write_csv(sf_data, "..sendis/data-raw/sf_longdata.csv")

###################################################@@
```

A dataset `sfcompo` has been extracted from the official SFCOMPO-2.0 database, containing data for 647 samples (This is lower than the official database of 750 samples because we choose to drop samples without uncertainty information, in order to have a consisten dataset without missing data). 


```{r, echo=FALSE, message=FALSE }

## Reading data already prepped
sf_data<-fread("../sendis/data-raw/sf_longdata.csv", 
               stringsAsFactors = TRUE)
subdf<-sf_data%>%
  select(-EPU, -EPUFISS, -EU235)
rownames(subdf)<-NULL
subdf<-subdf%>%
  column_to_rownames(var="SAMPLEID")
```

## Principal Component Analysis  

Principal component analysis methods (PCA) are commonly used dimensionality reduction techniques that transform a collection of correlated variables into a smaller number of uncorrelated variables (or features) called *principal components*. Principal components are linear combinations of the original variables that account for most of the variance in the observed data. In other words, PCA projects the entire dataset onto a lower-dimensional subspace of different features where the principal components are the eigenvectors forming the axes. 

An important question is the number of principal components that are needed to describe the new feature subspace. A useful measure is the so-called "explained variance," which can be calculated from the eigenvalues and tells us how much information (variance) can be attributed to each of the principal components.


PCA is good for data visualisation and visualising groups of observations that may be gathered in different clusters (categorisation); however, the interpretability of the physical meaning of principal components may be a tradeoff. 

```{r}

# for prcomp categorical variables cannot be used, and scaling data :
subdf2<-subdf%>% 
  select(-RTYPE, -U0, -U238, -Am242, -H3, -EBUP)%>%
  scale()%>%
  as.data.frame()
 

pca.out<-prcomp(subdf2, scale=TRUE, center = TRUE)

g<-fviz_pca_ind(pca.out,
                label = "none",
                habillage = subdf$RTYPE)
ggplotly(g)


d<-fviz_pca_biplot(pca.out, 
                   label = "none",
                   repel = TRUE)
d

```



## Random forest decision trees

 


```{r, echo=FALSE, message=FALSE }
# Splitting into training and test sets : 
set.seed(137)

n <- nrow(subdf)
shuffled_df <- subdf[sample(n),]

fraction_training<-0.845
train_indices <- 1:round(fraction_training * n)
test_indices <- (round(fraction_training* n) + 1):n

train <- shuffled_df[train_indices, ]
test <- shuffled_df[test_indices, ]
```

**`r nrow(test)`** observations are aleatorily chosen from the `sfcompo` data set : this will be our *test set*. The remaining `r nrow(train)` observations are our *training set*. 

### Training set 

The aleatory training dataset is chosen from the `r nrow(sf_data)` samples of our `sfcompo` dataset. 

```{r, histos, echo=FALSE}
require(gridExtra) 

#plot_ly(train, y=~RTYPE, type = "histogram")

g1<-ggplot(train, aes(x = RTYPE)) + geom_histogram(stat = "count")+
  theme_light()+
  coord_flip()

g2<-ggplot(test, aes(x = RTYPE)) + geom_histogram(stat = "count")+
  theme_light()+
  coord_flip()

grid.arrange(g1, g2, ncol=2)
#ggplot(test, aes(x = RTYPE)) + geom_histogram(stat = "count")


```




```{r, echo=FALSE, message=FALSE }

tree<-randomForest(RTYPE~., data=train,
                       ntree = 500, mtry =10,
                       na.action = na.omit)

#tree <- rpart(RTYPE~ ., data=train)
#fancyRpartPlot(tree)


# Draw the decision tree
# fancyRpartPlot(tree)

pred_tree<-predict(tree, test)
#summary(pred_tree)

conf<-table(test$RTYPE, pred_tree)
kable(conf)%>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))%>%
  row_spec(0, angle = -45)

acc<-sum(diag(conf))/sum(conf)


```

The calculated efficiency of the above algorith (true positives over total cases) is `r round(acc,3)*100` %. 



## Predictor of Burnup

```{r, eval=FALSE}
## try predict EBUP : 

train<-train%>%select(-RTYPE)
test<-test%>%select(-RTYPE)
tree<-randomForest(EBUP~., data=train,
                      ntree = 5000, mtry =50,
                      na.action = na.omit)
pred_tree<-predict(tree, test)

test<-test%>%
  mutate(SAMPLEID=rownames(.))
  

pred_df<-as.data.frame(pred_tree)
colnames(pred_df)<-c("PREDICTED")
pred_df<-pred_df%>%
  mutate(SAMPLEID=rownames(.))%>%
  #merge(test, by=c('SAMPLEID'), all.x = TRUE, all.y = FALSE)%>%
  merge(samples_info, by=c('SAMPLEID'), all.x = TRUE, all.y = FALSE)%>%
  select(SAMPLEID, RNAME, RTYPE, EBUP, PREDICTED)%>%
  mutate(RATIO=PREDICTED/EBUP)
  

ggplot(pred_df, aes(x=SAMPLEID, y=RATIO, color=RTYPE))+
  geom_point()

```







  
  
